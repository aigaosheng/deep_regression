#!/usr/bin/env python
# -*- coding: utf-8 -*-
from __future__ import absolute_import
import sys, os

#root module path.
#module_root = os.path.join(os.path.dirname(__file__), '..')
#sys.path.append(module_root)
#sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from configEnv import IS_VM

import csv
import shutil
import cPickle as pickle
#from pyarrow import deserialize_from
import numpy as np
import random
import datetime
import json
import argparse
import glob

import keras
from keras.models import load_model

sys.path.append(os.path.dirname(__file__))

from model.model import fit, scoreReport
from model.dnnModel import fit as fitDnn
from model.dnnModel import addUserDefinedMetric
from model.dnnModel import modelEval as modelEvalDnn
from meradio.featureExtractor import computeFeature, buildTrainInsts
from utils.clusterInst import clusterInstances
from model.dnnMetric import METRIC_LIST

import settings

SAVE_TEST_DETAIL = False


def run_evaluation(experiment_root_path, result_fold, site_name, train_dev_test_file, model_type_list, topic_model_type, is_rebuild_train_inst = True):
    '''
    Run model training and evaluating on dev/test set.
    Use pre-prepared memmap feature file, generated by featureExtractor 

    Input:
        experiment_root_path: save experient results into the site_name fold of the path
        site_name: site name
        train_dev_test_prefix_name: the prefix name of randomly split train/dev/test set, extension name default: _train.csv, _dev.csv, _test.csv
        is_rebuild_train_inst: True, rebuid training/dev/test instances for model learning and evaluation. If running many experiments on same data sets. So can set false after first time running to save time

    Model setting:
        model_type: regtree of scikit
        topic_model_type:  7/24 (group contentID based on publication day of week/hour of day)

    '''
    result_path = os.path.join(experiment_root_path, site_name, result_fold)
    if not os.path.exists(result_path):
        os.makedirs(result_path)
        print('INFO: create folder %s' % experiment_root_path)

    #Get path of train/dev/test, whose are named based on rule with same prefix name + _train, _dev, _test

    #load section names
    if settings.SEQ2INST_PARAM['is_section_as_feature']:
        section_name_file = os.path.join(experiment_root_path, site_name, site_name + '_sections.json')
        with open(section_name_file, 'rb') as i_fsection:
            names = json.load(i_fsection)
            section_name_map = {}
            for ky, v in names[1].iteritems():
                section_name_map[ky] = v['id']
    else:
        section_name_map = None
    #
    #result_path = os.path.join(experiment_root_path, site_name, result_fold)
    #if not os.path.exists(result_path):
    #    os.mkdir(result_path)
    #    print('INFO: create fold < %s > to save experimental results' % result_path)

    #file name to save instances to a file
    o_inst_file_train = train_dev_test_file['train']
    if 'dev' not in train_dev_test_file or train_dev_test_file['dev'] is None or train_dev_test_file['dev'] == '':
        o_inst_file_dev = None
    else:
        o_inst_file_dev = train_dev_test_file['dev']
    o_inst_file_test = train_dev_test_file['test']

    #batch experiments on different model methods
    for model_type in model_type_list:
        if topic_model_type:
            model_path = os.path.join(result_path, model_type + '_' + topic_model_type)
        else:
            model_path = os.path.join(result_path, model_type)
        if not os.path.exists(model_path):
            os.mkdir(model_path)
            print('INFO: create fold to save model type related results %s' % model_path)

        #Train model
        is_hour_metric = True
        o_save_model_file = os.path.join(model_path, 'pv_model.pkl')
        if model_type == 'dnn':
            fitDnn(o_inst_file_train, o_save_model_file, o_inst_file_dev, model_type, topic_model_type, is_hour_metric)
        else:
            #pass
            fit(o_inst_file_train, o_save_model_file, o_inst_file_dev, model_type, topic_model_type, is_hour_metric)

        #return
        
        x_test_meta_embed, x_test, y_test, x_test_meta, _ = computeFeature(o_inst_file_test, settings.IS_REMOVE_LOW_PV_VV_TEST)
        test_inst_index = clusterInstances(x_test_meta, x_test, y_test, method = topic_model_type)
        
        if model_type == 'dnn':
            model_files = glob.glob(o_save_model_file + '.*')
            meModel = {}
            
            nnConfig = settings.DNN_CONFIG
            nnConfig['i_dim'] = x_test.shape[1]
    
            addUserDefinedMetric()
            '''if not hasattr(keras.metrics, nnConfig['val_metric']):
                setattr(keras.metrics, nnConfig['val_metric'], METRIC_LIST[nnConfig['val_metric']])
            if not hasattr(keras.losses, nnConfig['loss']):
                setattr(keras.losses, nnConfig['loss'], METRIC_LIST[nnConfig['loss']])'''

            for fl in model_files:
                gkey = fl.split('.')[-1]
                meModel[gkey] = load_model(fl)
            print('****')
            '''featureMemmap_file_folder = os.path.join(os.path.dirname(o_inst_file_test), 'featuremap')
            if not os.path.exists(featureMemmap_file_folder):
                os.mkdir(featureMemmap_file_folder)
            featureMemmap_file = os.path.join(os.path.dirname(o_inst_file_test), 'featuremap', os.path.basename(o_inst_file_test))
            y_predict = np.memmap(featureMemmap_file + '.eval', dtype = 'float32', mode = 'w+', shape = eval_insts_y.shape)'''

            metric_score, (y_Truth, y_Predict) = modelEvalDnn(meModel, test_inst_index, x_test, y_test, None, x_test_meta_embed = x_test_meta_embed)
            x_Feat = x_test
            print('Total points %d' % y_Truth.size)
        else:
            with open(o_save_model_file, 'rb') as o_fmodel:
                meModel = pickle.load(o_fmodel)
                try:
                    featureModelSet = pickle.load(o_fmodel)
                except:
                    featureModelSet = None
                #xtransform = pickle.load(o_fmodel)
                metric_score, (x_Feat, y_Truth, y_Predict) = scoreReport(x_test, y_test, test_inst_index, meModel, model_type, topic_model_type, is_hour_metric, feature_model_set=featureModelSet)
            '''
            try:
                meModel, featureModelSet = pyarrow.deserialize_from(o_save_model_file)
                metric_score, (x_Feat, y_Truth, y_Predict) = scoreReport(test_insts_x, test_insts_y, test_inst_index, meModel, model_type, topic_model_type, is_hour_metric, feature_model_set=featureModelSet)
            except:
                raise Exception('Model file %s not found' % o_save_model_file)
            '''
        #remove memmap
        featureMemmap_file_folder = os.path.join(os.path.dirname(o_inst_file_test), 'featuremap')
        shutil.rmtree(featureMemmap_file_folder)
        
        correct_nozero_predict = np.sum(np.logical_and(y_Truth == y_Predict,y_Truth>0)) 
        total_nonzero = np.sum(y_Truth>0)
        print('Non-zeror correct predict %d ( %f )' % (correct_nozero_predict, correct_nozero_predict * 100.0 / total_nonzero))

        o_metric_file = 'metric_score_' + model_type + '_' + site_name + '.csv'
        metric_score_file = os.path.join(model_path, o_metric_file) #metric_score.csv')
        with open(metric_score_file, 'wb') as o_fscore:
            filednames = ['metric', 'average']
            metric_w = csv.DictWriter(o_fscore, fieldnames = filednames)
            metric_w.writeheader()  
            ss = {'average': metric_score['smape'], 'metric': 'smape'}
            metric_w.writerow(ss)
            ss = {'average': metric_score['rmse'], 'metric': 'rmse'}
            metric_w.writerow(ss)
            ss = {'average': metric_score['ase'], 'metric': 'ase'}
            metric_w.writerow(ss)
            ss = {'average': metric_score['r2'], 'metric': 'r2'}
            metric_w.writerow(ss)

            o_fscore.write('\n\nNon-zeror correct predict %d ( %f )' % (correct_nozero_predict, correct_nozero_predict * 100.0 / total_nonzero))
        



        if not SAVE_TEST_DETAIL:
            return 

        index2dh = {}
        if topic_model_type:
            for gkey, gidx in test_inst_index.iteritems():
                if gidx is None or len(gidx) == 0 or gkey == 'generic':
                    continue
                for k in gidx:
                    index2dh[k] = gkey
        else:
            for gkey, gidx in test_inst_index.iteritems():
                if gkey == 'generic':
                    for k in gidx:
                        index2dh[k] = gkey

        
        o_predict_file = 'predict_cmp_' + model_type + '_' + site_name + '.csv'
        cmp_predict_file = os.path.join(model_path, o_predict_file) #metric_score.csv')
        with open(cmp_predict_file, 'wb') as o_fscore:
            for k, r in enumerate(y_Truth):
                rt = [index2dh[k]] + [str(v) for v in r]
                ss = ' '.join(rt)
                o_fscore.write(ss + '\n')  
                rp = [index2dh[k]] + [str(v) for v in y_Predict[k]]
                ss = ' '.join(rp)
                o_fscore.write(ss + '\n')  
                o_fscore.write('****\n')
        
        #if not SAVE_TEST_DETAIL:
        #    return 

        o_feat_file = 'feature_' + model_type + '_' + site_name + '.csv'
        cmp_predict_file = os.path.join(model_path, o_feat_file) #metric_score.csv')
        with open(cmp_predict_file, 'wb') as o_fscore:
                for k, r in enumerate(y_Truth):
                    rp = [index2dh[k]] + [str(v) for v in x_Feat[k]]
                    ss = ' '.join(rp)
                    o_fscore.write(ss + '\n')  
                    rt = [index2dh[k]] + [str(v) for v in r]
                    ss = ' '.join(rt)
                    o_fscore.write(ss + '\n')  
                    o_fscore.write('****\n')  
        o_feat_file = 'smape_' + model_type + '_' + site_name + '.csv'
        cmp_predict_file = os.path.join(model_path, o_feat_file) #metric_score.csv')
        with open(cmp_predict_file, 'wb') as o_fscore:
            for gkey, gidx in test_inst_index.iteritems():
                if gidx is None or len(gidx) == 0:
                    continue
                #for k, r in enumerate(y_Truth):
                o_fscore.write(gkey + '\n')
                for k in gidx:
                    #for k, r in enumerate(y_Truth):
                    try:
                        rp = [str(v) for v in metric_score['smape_inst'][k]]
                        ss = ' '.join(rp)
                        o_fscore.write(ss + '\n')  
                    except:
                        break
        o_feat_file = 'abs_' + model_type + '_' + site_name + '.csv'
        cmp_predict_file = os.path.join(model_path, o_feat_file) #metric_score.csv')
        with open(cmp_predict_file, 'wb') as o_fscore:
            for gkey, gidx in test_inst_index.iteritems():
                if gidx is None or len(gidx) == 0:
                    continue
                #for k, r in enumerate(y_Truth):
                o_fscore.write(gkey + '\n')
                for k in gidx:
                    #for k, r in enumerate(y_Truth):
                    try:
                        rp = [str(v) for v in metric_score['ase_inst'][k]]
                        ss = ' '.join(rp)
                        o_fscore.write(ss + '\n')  
                    except:
                        break


        if not SAVE_TEST_DETAIL:
            return 

        index2dh = {}
        if topic_model_type:
            for gkey, gidx in test_inst_index.iteritems():
                if gidx is None or len(gidx) == 0 or gkey == 'generic':
                    continue
                for k in gidx:
                    index2dh[k] = gkey
        else:
            for gkey, gidx in test_inst_index.iteritems():
                if gkey == 'generic':
                    for k in gidx:
                        index2dh[k] = gkey

        
        o_predict_file = 'predict_cmp_' + model_type + '_' + site_name + '.csv'
        cmp_predict_file = os.path.join(model_path, o_predict_file) #metric_score.csv')
        with open(cmp_predict_file, 'wb') as o_fscore:
            for k, r in enumerate(y_Truth):
                rt = [index2dh[k]] + [str(v) for v in r]
                ss = ' '.join(rt)
                o_fscore.write(ss + '\n')  
                rp = [index2dh[k]] + [str(v) for v in y_Predict[k]]
                ss = ' '.join(rp)
                o_fscore.write(ss + '\n')  
                o_fscore.write('****\n')
        
        #if not SAVE_TEST_DETAIL:
        #    return 

        o_feat_file = 'feature_' + model_type + '_' + site_name + '.csv'
        cmp_predict_file = os.path.join(model_path, o_feat_file) #metric_score.csv')
        with open(cmp_predict_file, 'wb') as o_fscore:
                for k, r in enumerate(y_Truth):
                    rp = [index2dh[k]] + [str(v) for v in x_Feat[k]]
                    ss = ' '.join(rp)
                    o_fscore.write(ss + '\n')  
                    rt = [index2dh[k]] + [str(v) for v in r]
                    ss = ' '.join(rt)
                    o_fscore.write(ss + '\n')  
                    o_fscore.write('****\n')  
        o_feat_file = 'smape_' + model_type + '_' + site_name + '.csv'
        cmp_predict_file = os.path.join(model_path, o_feat_file) #metric_score.csv')
        with open(cmp_predict_file, 'wb') as o_fscore:
            for gkey, gidx in test_inst_index.iteritems():
                if gidx is None or len(gidx) == 0:
                    continue
                #for k, r in enumerate(y_Truth):
                o_fscore.write(gkey + '\n')
                for k in gidx:
                    #for k, r in enumerate(y_Truth):
                    try:
                        rp = [str(v) for v in metric_score['smape_inst'][k]]
                        ss = ' '.join(rp)
                        o_fscore.write(ss + '\n')  
                    except:
                        break
        o_feat_file = 'abs_' + model_type + '_' + site_name + '.csv'
        cmp_predict_file = os.path.join(model_path, o_feat_file) #metric_score.csv')
        with open(cmp_predict_file, 'wb') as o_fscore:
            for gkey, gidx in test_inst_index.iteritems():
                if gidx is None or len(gidx) == 0:
                    continue
                #for k, r in enumerate(y_Truth):
                o_fscore.write(gkey + '\n')
                for k in gidx:
                    #for k, r in enumerate(y_Truth):
                    try:
                        rp = [str(v) for v in metric_score['ase_inst'][k]]
                        ss = ' '.join(rp)
                        o_fscore.write(ss + '\n')  
                    except:
                        break


if __name__ == '__main__':
    '''
    python test.py --root ../../CorpusRV/EXP --output results_jan3019 --property meradio --model dnn --train ../../CorpusRV/Clean/Split/radioview_201810400_2019012023_feature_meradio.memmap_train --dev ../../CorpusRV/Clean/Split/radioview_201810400_2019012023_feature_meradio.memmap_dev --test ../../CorpusRV/Clean/Split/radioview_201810400_2019012023_feature_meradio.memmap_test

    python test.py --root ../../CorpusRV/EXP --output results_jan3019 --property podcast --model dnn --train ../../CorpusRV/Clean/Split/radioview_201810400_2019012023_feature_podcast.memmap_train --dev ../../CorpusRV/Clean/Split/radioview_201810400_2019012023_feature_podcast.memmap_dev --test ../../CorpusRV/Clean/Split/radioview_201810400_2019012023_feature_podcast.memmap_test
    '''

    parser = argparse.ArgumentParser(description=' +++  Offline train and evaluate traffic forcasting model. The full path to save experiment output is root/output/property/model(dnn)/model-and-other-results-here +++ ')
    parser.add_argument('--root',  action='store', required=False, help='Root path to save experiment output')
    parser.add_argument('--output',  action='store', required=False, help='fold name, under root path, to save experiment output')
    parser.add_argument('--property',  action='store', required=False, help='property name list, also sub-fold name under output, to save property model and experiment output')
    parser.add_argument('--model',  action='store', required=False, nargs = '+', help='model type list to be trained and evaluated. Currently only dnn')
    parser.add_argument('--train',  action='store', required=False, help='train data file - memmap format')
    parser.add_argument('--test',  action='store', required=False, help='test data file - memmap format')
    parser.add_argument('--dev',  action='store', required=False, help='development data file - memmap format. If NULL or empty, use random split from train samples as validate data')


    try:
        arg_config = parser.parse_args(sys.argv[1:])
        if arg_config.root is None:
            arg_config.root = '../../CorpusRV/EXP/'
        if arg_config.output is None:
            arg_config.output = 'results_jan3019' #'results_debug'
        if arg_config.property is None:
            arg_config.property = 'podcast'
        if arg_config.model is None:
            arg_config.model = ['dnn']
        if arg_config.train is None:
            arg_config.train = '../../CorpusRV/Clean/Split/radioview_201810400_2019012023_feature_podcast.memmap_train'
        if arg_config.dev is None:
            arg_config.dev =             '../../CorpusRV/Clean/Split/radioview_201810400_2019012023_feature_podcast.memmap_dev'
        if arg_config.test is None:
            arg_config.test = '../../CorpusRV/Clean/Split/radioview_201810400_2019012023_feature_podcast.memmap_test'

        print(arg_config) 

        split_train_dev_test_files = {
            'train': arg_config.train,
            'dev': arg_config.dev,
            'test': arg_config.test,
        }
        topic_model_type =  None #24-inst'# #['7-24-inst'] #, '7-24'] #None  #-inst' #None #

        assert(arg_config.property in settings.SITE_NAMES)
        run_evaluation(arg_config.root, arg_config.output, arg_config.property, split_train_dev_test_files, arg_config.model, topic_model_type, is_rebuild_train_inst=True)

    except Exception as e:
        print(e)